---
layout: article
title: 点互信息和互信息
---

## 点互信息PMI

点互信息PMI\(Pointwise Mutual Information\)这个指标来衡量两个事物之间的相关性\(比如两个词\)。

其公式如下：

\$\$  
PMI\(x:y\) = log \\frac\{p\(x,y\)\}\{p\(x\)p\(y\)\} = log \\frac\{p\(x|y\)\}\{p\(x\)\} = log \\frac\{p\(y|x\)\}\{p\(y\)\}  
\$\$

<!--more-->

- 在概率论中，如果\$x\$跟\$y\$不相关，则\$p\(x,y\)=p\(x\)p\(y\)\$。如果\$x\$跟\$y\$相关性越大，则\$p\(x, y\)\$就相比于\$p\(x\)p\(y\)\$越大。在\$y\$出现的情况下\$x\$出现的条件概率\$p\(x|y\)\$除以\$x\$本身出现的概率\$p\(x\)\$，就表示\$x\$跟\$y\$的相关程度。
- 比如，想衡量\$like\$这个词的极性（正向情感还是负向情感）。我们可以预先挑选一些正向情感的词，比如\$good\$。然后我们算\$like\$跟\$good\$的PMI。

## 互信息MI

点互信息PMI其实就是从信息论里面的互信息这个概念里面衍生出来的。

\$\$  
I\(X;Y\) = \\sum\_\{x∈X\}\{\\sum\_\{y∈Y\}\{p\(x,y\)=log \\frac\{p\(x,y\)\}\{p\(x\)p\(y\)\}\}\}  
\$\$

- 其衡量的是两个随机变量之间的相关性，即一个随机变量中包含的关于另一个随机变量的信息量。所谓的随机变量，即随机试验结果的量的表示，可以简单理解为按照一个概率分布进行取值的变量，比如随机抽查的一个人的身高就是一个随机变量。

- 可以看出，互信息其实就是对X和Y的所有可能的取值情况的点互信息PMI的加权和。因此，点互信息这个名字还是很形象的。