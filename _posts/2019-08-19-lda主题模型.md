---
layout: post
title: LDA主题模型
---

LAD\(Latent Dirichlet allocation\)指的是隐含狄利克雷分布，是一种主题模型，可以将文档的主题按照概率分布的形式给出，是一种无监督学习方式。

LDA 中，生成文档的过程如下：

1.  按照先验概率\$p\(d\_i\)\$选择一篇文档\$d\_i\$
2.  从Dirichlet分布\$α\$中取样生成文档\$d\_i\$的主题分布\$θ\_i\$，主题分布\$θ\_i\$由超参数为\$α\$的Dirichlet分布生成

3.  从主题的多项式分布\$θ_i\$中取样生成文档\$d\_i\$第 j 个词的主题\$z_\{i,j\}\$

4.  从Dirichlet分布\$β\$中取样生成主题\$z\_\{i,j\}\$对应的词语分布\$\\phi\_\{z\_\{i,j\}\}\$，词语分布\$\\phi\_\{z\_\{i,j\}\}\$由参数为\$β\$的Dirichlet分布生成

5.  从词语的多项式分布\$\\phi\_\{z\_\{i,j\}\}\$中采样最终生成词语\$w\_\{i,j\}\$

LDA 在 PLSA 的基础上，为主题分布和词分布分别加了两个 Dirichlet 先验。

<!--more-->

因此整个模型中所有可见变量以及隐藏变量的联合分布是：  
![](http://39.106.118.77/wp-content/uploads/2019/08/f320bdacad2478a02b1a03628c09f20e.png)

最终一篇文档的单词分布的最大似然估计可以通过将上式的 \$\\theta _\{i\}\$以及 \$\\Phi \$进行积分和对\$z_\{i\}\$进行求和得到  
![](http://39.106.118.77/wp-content/uploads/2019/08/e2cde8612844312c9980b7569cab6595.png)

根据 \$p\(w\_\{i\}|\\alpha ,\\beta \)\$的最大似然估计，最终可以通过吉布斯采样等方法估计出模型中的参数。

# 后续....