---
layout: article
title: 交叉熵(cross entropy)
tags:
    - Deep Learning
---

## 1\. 定义

基于概率分布$p$和$q$的交叉熵定义为：

$$  
H(p,q) = E_{p}[-log\ q] = H(p) + D_{KL}(p||q)  
$$

其中 $H(p)$是$p$的熵， $D_{KL}(p||q)$是从 $p$到$q$的KL散度(也被称为p相对于q的相对熵)。

对于离散分布$p$和$q$，这意味着：

$$  
H(p,q) = \sum_{x}{p(x)\ log\ q(x)}  
$$

$p$ 是信息的实际分布，而$q$则为错误分布。

<!--more-->  
<!--more Keep reading this post-->  
s

## 2\. 信息量

首先是信息量。假设我们听到了两件事，分别如下：  
事件A：巴西队进入了2018世界杯决赛圈。  
事件B：中国队进入了2018世界杯决赛圈。  
仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。

假设$X$是一个离散型随机变量，其取值集合为$\chi$,概率分布函数$p(x) = Pr(X\ =\ x),x∈\chi$,则定义事件$X=x0$的信息量为：

$$  
I(x_0)\ = -log(p(x_0))  
$$

## 3\. 熵

考虑另一个问题，对于某个事件，有$n$种可能性，每一种可能性都有一个概率$p(xi)$  
这样就可以计算出某一种可能性的信息量。举一个例子，假设你拿出了你的电脑，按下开关，会有三种可能性，下表列出了每一种可能的概率及其对应的信息量

| 序号 | 事件 | 概率p | 信息量I |
| --- | --- | --- | --- |
| A | 电脑正常开机 | 0.7 | $-log(p(A))=0.36$ |
| B | 电脑无法开机 | 0.2 | $-log(p(B))=1.61$ |
| C | 电脑爆炸了 | 0.1 | $-log(p(C))=2.30$ |

我们现在有了信息量的定义，而熵用来表示所有信息量的期望，即：

$$  
H(X) = - \sum_{i=1}^{n}{p(x_i)\ log(p(x_i))}  
$$

其中n代表所有的n种可能性，所以上面的问题结果就是：

$$  
H(X) =−[p(A)log(p(A))+p(B)log(p(B))+p(C))log(p(C))] \  
= 0.7×0.36+0.2×1.61+0.1×2.30 = 0.804  
$$

## 4\. 相对熵（KL散度）

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]  
直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。

KL散度的计算公式：

$$  
D_{KL}{(p||q)} = \sum_{i=1}^{n}{p(x_i) log{\frac{p(x_i)}{q(x_i)}}}  
$$

$n$为事件的所有可能性。  
$D_{KL}$的值越小，表示$q$分布和$p$分布越接近

## 5\. 交叉熵

对相对熵公式进行展开，得到：

$$  
D_{KL}{(p||q)} = \sum_{i=1}^{n}{p(x_i) log(p(x_i))} - \sum_{i=1}^{n}{p(x_i) log(q(x_i))} \  
= -H(p(x)) + [- \sum_{i=1}^{n}{p(x_i)log(q(x_i))}]  
$$

等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：

$$  
H(p,\ q) = - \sum_{i=1}^{n}{p(x_i)log(q(x_i))}  
$$

在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即$D_{KL}(y || \hat y)$，由于KL散度中的前一部分$−H(y)$不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。

## 6\. 估计

在大多数情况下，我们需要在不知道分布$p$的情况下计算其交叉熵。例如在语言模型中, 我们基于训练集 $T$创建了一个语言模型, 而在测试集合上通过其交叉熵来评估该模型的准确率。 $p$是语料中词汇的真实分布，而$q$是我们获得的语言模型预测的词汇分布。由于真实分布是未知的，我们不能直接计算交叉熵。在这种情况下，我们可以通过下式来估计交叉熵:

$$  
H(T,\ q) = - \sum_{i=1}^{N}{\frac{1}{N}\ log_{2}{q(x_{i})}}  
$$

$N$是测试集大小， $q(x)$是在训练集上估计的事件 $x$发生的概率。我们假设训练集是从 $p(x)$的真实采样，则此方法获得的是真实交叉熵的蒙特卡洛估计。

## 7\. 交叉熵在机器学习中的应用

### 7.1 交叉熵在单分类问题中的使用

这里的单类别是指，每一张图像样本只能有一个类别，比如只能是狗或只能是猫。  
交叉熵在单分类问题上基本是标配的方法：

$$  
loss = - \sum_{i=1}^{n}{y_{i} log(\hat y_{i})}  
$$

对应一个batch的loss就是:

$$  
loss = - \frac{1}{m} \sum_{j=1}^{m}\sum_{i=1}^{n}{y_{ji} log(\hat y_{ji})}  
$$

### 7.2 交叉熵在多分类问题中的使用

这里的多类别是指，每一张图像样本可以有多个类别，比如同时包含一只猫和一只狗  
和单分类问题的标签不同，多分类的标签是n-hot。

值得注意的是，这里的Pred不再是通过softmax计算的了，这里采用的是sigmoid。将每一个节点的输出归一化到[0,1]之间。所有Pred值的和也不再为1。换句话说，就是每一个Label都是独立分布的，相互之间没有影响。所以交叉熵在这里是单独对每一个节点进行计算，每一个节点只有两种可能值，所以是一个二项分布。前面说过对于二项分布这种特殊的分布，熵的计算可以进行简化。

同样的，交叉熵的计算也可以简化，即：

$$  
loss=−ylog(ŷ )−(1−y)log(1−ŷ )  
$$

注意，上式只是针对一个节点的计算公式。这一点一定要和单分类loss区分开来。  
例子中可以计算为：

$$  
loss_猫 = −0×log(0.1)−(1−0)log(1−0.1)=−log(0.9)\  
loss_蛙 = −1×log(0.7)−(1−1)log(1−0.7)=−log(0.7)\  
loss_鼠 = −1×log(0.8)−(1−1)log(1−0.8)=−log(0.8)  
$$

单张样本的loss即为$loss=loss_猫+loss_蛙+loss_鼠$  
每一个batch的loss就是：

$$  
loss = \frac{1}{m} \sum_{j=1}^{m}\sum_{i=1}^{n}{-y_{ji} log(\hat y_{ji})-(1-y_{ji}) log(1- \hat y_{ji})}  
$$

式中m为当前batch中的样本量，n为类别数。