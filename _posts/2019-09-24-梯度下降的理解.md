---
layout: article
title: 梯度下降的理解
tags:
    - Deep Learning
---
gradient descent(梯度下降)是一种优化算法，在machine learning中经常用来优化目标函数。

假设$X=(x_1,x_2 )$代表身高体重数据，$x_1$为身高，$x_2$为体重，$Y$表示类标号，当$y=1$的时候表示男生，当$y=-1$的时候表示女生。如果能找到一个函数$f(X)$能够尽可能准确描述这些数据，使得给出任意一个$X$，能够预测出这个人是男生还是女生。

这里可以假设$f(X)$为一个线性函数：

$$
\begin{aligned}  
f(X)=θ_1 x_1 + θ_2 x_2 + θ_3  
\end{aligned}  
$$

<!--more-->

那么，如何计算误差呢？对于男生而言，误差就是$f(X)$与1的差距，对于女生而言，误差就是$f(X)$与-1的差距，总的误差函数(cost function)可以定义如下：

$$
\begin{aligned}  
L(θ)= \frac{1}{2} \sum_{i=1}^{n}[f(X^i)-Y^i]^2  
\end{aligned}  
$$

其中，$X_i$是第i个样本，$Y(i)$是第i个样本对应的类标号。  
现在，要使得cost function最小，也就是求出当$θ_1,θ_2,θ_3,$取什么值时，cost最小。

梯度是一个向量，梯度所指的方向就是函数增长最快的方向，我们沿着梯度方向调整参数，就可以使得cost变化得最快。但是梯度所指的方向是函数增长最快的方向，我们要的是cost减少而不是增加，所以，只要取反就可以了，也就是说，梯度所指的反方向就是函数值下降最快的方向，因此沿着梯度的反方向调整参数，就能让cost function以最快的速度下降。

要求得$L(θ)$的梯度，就要对参数$θ_1,θ_2,θ_3,$求偏导：

$$
\begin{aligned}  
\frac{\partial L}{\partial θ_1} = \sum_{i=1}^{n} x_1^i [f(X^i)-Y^i] \  
\frac{\partial L}{\partial θ_2} = \sum_{i=1}^{n}x_2^i [f(X^i)-Y^i] \  
\frac{\partial L}{\partial θ_3} = \sum_{i=1}^{n} [f(X^i)-Y^i]  
\end{aligned}\tag{1.3}
$$

以上为$θ_1,θ_2,θ_3,$的损失函数的梯度。

接下来，使用学习率$\eta$(learning rate)来控制每一步走的大小，并乘以损失函数的梯度，得到下降的距离：

$$
\begin{aligned}  
θ_1 = θ_1 - \eta · \frac{\partial L}{\partial θ_1} \  
θ_2 = θ_2 - \eta · \frac{\partial L}{\partial θ_2} \  
θ_3 = θ_3 - \eta · \frac{\partial L}{\partial θ_3}  
\end{aligned}\tag{1.4}  
$$

每次迭代时，判断$θ_1,θ_2,θ_3,$的梯度下降的距离是否小于终止距离$\varepsilon$，如果是的话，则算法终止，否则重复步骤(1.4)，直到满足要求。