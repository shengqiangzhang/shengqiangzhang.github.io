---
layout: article
title: 深度学习-常见优化器算法
tags:
    - Deep Learning
---

## Batch Gradient Descent\(BGD\)

BGD 采用整个训练集的数据来计算 cost function 对参数的梯度：

\$\$  
θ = θ \- \\eta \\cdot \\bigtriangledown_\{θ\}J\(θ\)  
\$\$

```python
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
```

我们会事先定义一个迭代次数 epoch，首先计算梯度向量 params_grad，然后沿着梯度的方向更新参数 params，learning rate 决定了我们每一步迈多大。

Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。

<!--more-->

## Stochastic Gradient Descent\(SGD\)

梯度更新规则:

和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，**而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。**

\$\$  
θ = θ \- \\eta \\cdot \\bigtriangledown_\{θ\}J\(θ;x\^\{\(i\)\},y\^\{\(i\)\}\)  
\$\$

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad
```

看代码，可以看到区别，就是整体数据集是个循环，其中对每个样本进行一次参数更新。

![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310214248005-2068714250.png)

随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况，那么可能只用其中部分的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。缺点是SGD的噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。所以虽然训练速度快，但是准确度下降，并不是全局最优。**虽然包含一定的随机性，但是从期望上来看，它是等于正确的导数的。**

**缺点：**  
SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。LR会随着更新的次数逐渐变小。

## Mini-Batch Gradient Descent\(MBGD\)

梯度更新规则：

MBGD 每一次利用一小批样本，即 n 个样本进行计算，**这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。**

\$\$  
θ = θ \- \\eta \\cdot \\bigtriangledown_\{θ\}J\(θ;x\^\{\(i:i+n\)\},y\^\{\(i:i+n\)\}\)  
\$\$

和 SGD 的区别是每一次循环不是作用于每个样本，而是具有 n\(50～256\) 个样本的批次。

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad
```

**缺点：**

MBGD不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。）

后续所需补上的笔记...

## Momentum

## NAG

## Adagrad

## Adadelta

## RMSprop

## Adam